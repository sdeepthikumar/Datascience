{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "trainX, testX, trainY, testY = train_test_split(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pipeline = make_pipeline(StandardScaler(),RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pipeline.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 8, 4, 7, 9, 0, 7, 3, 2, 8, 1, 3, 8, 8, 6, 6, 3, 3, 2, 2, 7,\n",
       "       1, 6, 9, 9, 7, 6, 6, 2, 1, 2, 1, 1, 5, 3, 3, 6, 0, 5, 5, 3, 9, 7,\n",
       "       9, 7, 2, 6, 7, 7, 7, 3, 2, 1, 7, 7, 5, 9, 2, 7, 2, 2, 0, 2, 9, 6,\n",
       "       8, 3, 5, 3, 5, 3, 1, 4, 5, 7, 9, 7, 6, 1, 4, 9, 1, 7, 5, 9, 9, 6,\n",
       "       0, 4, 1, 8, 1, 9, 7, 5, 1, 4, 8, 5, 1, 8, 1, 3, 5, 0, 6, 1, 2, 5,\n",
       "       3, 0, 5, 9, 9, 7, 3, 1, 4, 9, 9, 2, 8, 1, 2, 8, 2, 5, 6, 4, 0, 7,\n",
       "       5, 8, 3, 5, 5, 9, 0, 9, 1, 9, 0, 7, 6, 7, 0, 0, 4, 6, 1, 8, 7, 8,\n",
       "       7, 5, 8, 5, 2, 5, 3, 7, 8, 3, 8, 0, 2, 6, 2, 8, 3, 0, 2, 7, 7, 1,\n",
       "       3, 4, 0, 3, 7, 9, 1, 5, 3, 2, 8, 5, 2, 2, 7, 4, 7, 6, 0, 2, 5, 2,\n",
       "       8, 1, 6, 4, 3, 6, 1, 0, 1, 5, 8, 9, 3, 9, 2, 5, 0, 5, 5, 1, 4, 3,\n",
       "       3, 1, 5, 1, 6, 0, 6, 0, 5, 5, 2, 2, 0, 2, 8, 4, 5, 1, 4, 7, 3, 4,\n",
       "       1, 7, 2, 9, 3, 0, 5, 5, 2, 3, 0, 0, 2, 3, 4, 3, 1, 8, 4, 6, 4, 6,\n",
       "       4, 6, 2, 7, 7, 3, 0, 2, 5, 5, 4, 3, 7, 8, 7, 0, 0, 0, 0, 1, 9, 5,\n",
       "       6, 5, 6, 1, 9, 9, 0, 5, 3, 5, 8, 2, 8, 9, 1, 3, 1, 3, 2, 4, 7, 5,\n",
       "       1, 7, 6, 3, 2, 1, 7, 3, 7, 1, 9, 4, 4, 9, 7, 5, 1, 1, 3, 1, 6, 1,\n",
       "       2, 4, 1, 7, 1, 8, 0, 8, 5, 4, 9, 7, 0, 1, 8, 9, 0, 6, 3, 8, 0, 2,\n",
       "       4, 2, 5, 6, 2, 2, 5, 2, 5, 4, 7, 8, 6, 4, 6, 1, 5, 3, 9, 0, 7, 8,\n",
       "       9, 3, 1, 8, 6, 1, 9, 0, 6, 0, 5, 8, 6, 1, 0, 0, 2, 9, 9, 3, 2, 1,\n",
       "       0, 3, 7, 6, 4, 1, 6, 9, 5, 9, 7, 1, 3, 5, 0, 8, 8, 0, 6, 3, 7, 5,\n",
       "       2, 6, 7, 9, 2, 7, 9, 1, 8, 7, 5, 9, 8, 6, 9, 5, 6, 8, 7, 8, 8, 6,\n",
       "       1, 6, 2, 5, 9, 4, 5, 1, 2, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pipeline.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler', StandardScaler()),\n",
       " ('randomforestclassifier', RandomForestClassifier())]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-207b2734ccf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdigit_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "digit_pipeline.steps[1].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pipeline.steps[1][1].feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting Pipeline with Hyper-parameter Tuning\n",
    "\n",
    "    We could determine the best combination of hyper-parameters for preprocessor & estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pipeline = make_pipeline(StandardScaler(),SelectKBest(k=20,score_func=f_classif),RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanu menon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: UserWarning: Features [ 0 32 39] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\sanu menon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('selectkbest', SelectKBest(k=20)),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pipeline.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler', StandardScaler()),\n",
       " ('selectkbest', SelectKBest(k=20)),\n",
       " ('randomforestclassifier', RandomForestClassifier())]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'selectkbest__k':[10,50,70],'randomforestclassifier__n_estimators':[500,600,700]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(digit_pipeline,param_grid= params,cv = 5, n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanu menon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: UserWarning: Features [ 0 32 39] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\sanu menon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('selectkbest', SelectKBest(k=20)),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=3,\n",
       "             param_grid={'randomforestclassifier__n_estimators': [500, 600,\n",
       "                                                                  700],\n",
       "                         'selectkbest__k': [10, 50, 70]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747459727385378"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 500, 'selectkbest__k': 50}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('selectkbest', SelectKBest(k=50)),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_estimators=500))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-fc6644270a6f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-fc6644270a6f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Column Transformer for dealing with hetrogenous data\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Column Transformer for dealing with hetrogenous data\n",
    "\n",
    "    Regular Pipeline intends to do same processing for all the columns\n",
    "    This doesn't work for hetrogenous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data = pd.read_csv('https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/HR_comma_sep.csv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data.rename(columns={'sales':'dept'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = hr_data.drop(columns=['left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = hr_data.left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "satisfaction_level & last_evaluation don't need preprocessing\n",
    "number_project,average_montly_hours,time_spend_company,Work_accident,promotion_last_5years need MinMaxScaler\n",
    "dept & Salary need OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = feature_data.select_dtypes(include=['object'])\n",
    "int_data = feature_data.select_dtypes(include=['int64'])\n",
    "float_data = feature_data.select_dtypes(include=['float64']) #doesnt require scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(OrdinalEncoder())\n",
    "int_pipeline = make_pipeline(MinMaxScaler(),SelectKBest(k=5,score_func=f_classif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if missing values are there:\n",
    "#cat_pipeline = make_pipeline(SimpleImputer(),OrdinalEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_pipeline = make_column_transformer(\n",
    "    (cat_pipeline,cat_data.columns),\n",
    "    (int_pipeline,int_data.columns),\n",
    "    remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessor_pipeline,RandomForestClassifier())\n",
    "\n",
    "trainX,testX, trainY, testY = train_test_split(feature_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(testX[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'columntransformer__pipeline-2__selectkbest__k':[4,5,6],\n",
    "          'randomforestclassifier__n_estimators':[50,100,200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline,param_grid=params,cv=5,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Disadvantages of Pipeline\n",
    "\n",
    "    Doesn't support Online Learning\n",
    "    Online Learning will be discussed later\n",
    "\n",
    "Dealing with imbalanced data in pipeline\n",
    "\n",
    "    Use imblearn make_pipeline rather than scikit make_pipeline as RandomOverSampler is not supported in scikit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This make_pipeline is not scikit pipeline but imblearn pipeline which support Oversampler as part of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_pipeline = make_pipeline(preprocessor_pipeline,RandomOverSampler,RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
