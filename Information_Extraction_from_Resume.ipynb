{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started job with id: cc7509c971d43ee032e49ed9bce751b2a0d4f256dad1d0b6c16d9e496b51a1a7\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "writing file completed  resume_1.txt\n",
      "Started job with id: edd2d488d88aedd3e013b6c574b7a3fefa161db04f63987325d642f69451bca5\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "writing file completed  resume_2.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "def startJob(s3BucketName, objectName):\n",
    "    response = None\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    response = client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': objectName\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return response[\"JobId\"]\n",
    "\n",
    "def isJobComplete(jobId):\n",
    "    # For production use cases, use SNS based notification \n",
    "    # Details at: https://docs.aws.amazon.com/textract/latest/dg/api-async.html\n",
    "    time.sleep(5)\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "\n",
    "    return status\n",
    "\n",
    "def getJobResults(jobId):\n",
    "\n",
    "    pages = []\n",
    "\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    \n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "\n",
    "    while(nextToken):\n",
    "\n",
    "        response = client.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "\n",
    "    return pages\n",
    "\n",
    "# Document\n",
    "\n",
    "\n",
    "s3BucketName = \"textract-console-ap-south-1-f12c9455-7ed3-4350-9a19-89a52cb444a\"\n",
    "\n",
    "\n",
    "client = boto3.client('s3')\n",
    "paginator = client.get_paginator('list_objects_v2')\n",
    "result = paginator.paginate(Bucket=s3BucketName)\n",
    "list_of_documents=[]\n",
    "for page in result:\n",
    "    if \"Contents\" in page:\n",
    "        for key in page[ \"Contents\" ]:\n",
    "            keyString = key[ \"Key\" ]\n",
    "            #print (keyString)\n",
    "            list_of_documents.append(keyString)\n",
    "filecounter=1\n",
    "for documentName in list_of_documents:\n",
    "    jobId = startJob(s3BucketName, documentName)\n",
    "    print(\"Started job with id: {}\".format(jobId))\n",
    "    if(isJobComplete(jobId)):\n",
    "        response = getJobResults(jobId)\n",
    "\n",
    "    #print(response)\n",
    "    file = []\n",
    "    # Print detected text\n",
    "    for resultPage in response:\n",
    "        for item in resultPage[\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "              #  print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')\n",
    "                file.append(item[\"Text\"])\n",
    "    # with open(\"response3.txt\", \"w\") as f:\n",
    "    #     f.write(str(file))\n",
    "    #     f.close()\n",
    "    f_name = 'resume'+'_'+str(filecounter)+'.txt'\n",
    "    filecounter+=filecounter\n",
    "    with open(f_name, 'w') as f:\n",
    "        for x in file:\n",
    "            s = \"\".join(map(str, x))\n",
    "            f.write(s+'\\n')\n",
    "        f.close()\n",
    "    print ( \"writing file completed \" ,f_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "import spacy\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_1.txt\n",
      "********************************\n",
      "*********Using Spacy************\n",
      "Krishna Sai Gandhi Group Of Institutions\n",
      "GPA 68\n",
      "Saraswathi Junior\n",
      "ES6 Backend\n",
      "STATE MANAGEMENT\n",
      "25th\n",
      "Microsoft\n",
      "React CloudArmor Supporting Reputation Based Trust Management for Cloud Services Trust Management\n",
      "Privacy Security and Availability\n",
      "*********Using RegexpParser Pos tagging************\n",
      "GPA 8 3\n",
      "Projects Women\n",
      "Services Trust\n",
      "resume_2.txt\n",
      "********************************\n",
      "*********Using Spacy************\n",
      "February 2019\n",
      "March 2019\n",
      "July 2016 2018\n",
      "American Telephone Telegraph Company\n",
      "National Institute of Designing Exam\n",
      "May 2012 June 2016\n",
      "March 2010\n",
      "April 2012 11th and 12th\n",
      "March 2010\n",
      "April 2012 10\n",
      "Sree Ayyappa Public School Bokaro Bhilai\n",
      "*********Using RegexpParser Pos tagging************\n",
      "February 2019 March 2019\n",
      "July 2016 2018\n",
      "May 2012 June 2016\n",
      "Bhilai 8 6\n",
      "March 2010 April 2012\n",
      "Bhilai 78 2\n",
      "March 2010 April 2012\n",
      "Bhilai 9 8\n"
     ]
    }
   ],
   "source": [
    "def load_files():\n",
    "    path = \"E:\\DataScience\\Job_Assignments\"\n",
    "    files = []\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path,i)) and i.startswith('resume'):\n",
    "            files.append(i)\n",
    "    return files\n",
    "\n",
    "def preprocess(file_content):\n",
    "    c = file_content.split('\\n')\n",
    "    p = string.punctuation\n",
    "    t = str.maketrans(p,len(p)*' ')\n",
    "    l=''\n",
    "    for x in c:\n",
    "        s = x.translate(t) \n",
    "        #print(s)\n",
    "        words = word_tokenize(s)\n",
    "        words = [w.strip() for w in words]\n",
    "        for w in words:\n",
    "            l = l+' ' +w\n",
    "    return l\n",
    "\n",
    "file_list =   load_files()\n",
    "for filename_read in file_list:\n",
    "    fp = open(filename_read,\"r\",encoding=\"utf-8\")\n",
    "    file_content = fp.read()\n",
    "    print(filename_read)\n",
    "    print('********************************')\n",
    "    \n",
    "    p_text = preprocess(file_content)\n",
    "    \n",
    "    #using Spacy\n",
    "    print('*********Using Spacy************')\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(p_text)\n",
    "    for x in doc.ents:\n",
    "        if len(x.text)>3 and (x.label_ == 'ORG'or x.label_ =='DATE'):\n",
    "            print(x.text) \n",
    "    \n",
    "    print('*********Using RegexpParser Pos tagging************')\n",
    "    pattern = r\"\"\"\n",
    "            Date : {<NNP><CD><NNP>?<CD>}\n",
    "           ORG : {<NNP.>+<IN.*>?<NNP.*>?}\n",
    "           \"\"\"\n",
    "    \n",
    "    word_token = nltk.word_tokenize(p_text)\n",
    "    tagged_words = nltk.pos_tag(word_token)\n",
    "    \n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    cs = cp.parse(tagged_words)\n",
    "    \n",
    "    for a in cs:\n",
    "        if isinstance(a, nltk.tree.Tree):\n",
    "            if a.label() == \"Date\" or a.label()=='ORG':\n",
    "                print(\" \".join([lf[0] for lf in a.leaves()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
